{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **TASK**\n",
        "\n",
        "Develop a Retrieval-Augmented Generation (RAG) model for a Question Answering (QA) bot that can\n",
        "process financial terms and insights from a Profit & Loss (P&L) table extracted from PDF documents.\n",
        "The QA bot should retrieve relevant information related to income, expenses, profit margins, and\n",
        "other key financial metrics from the provided P&L table and generate accurate and coherent\n",
        "responses.\n",
        "Task Requirements:\n",
        "\n",
        "● Implement a RAG-based model to handle questions related to a P&L table extracted from\n",
        "PDF documents.\n",
        "\n",
        "● Use a vector database (such as Pinecone) to store and retrieve document embeddings of\n",
        "financial terms and data points efficiently.\n",
        "\n",
        "● Parse P&L data from PDF documents into a structured format, such as tables or key-value\n",
        "pairs, before storing embeddings.\n",
        "\n",
        "● Test the model with several financial queries and show how accurately it retrieves and\n",
        "generates responses from the dataset.\n"
      ],
      "metadata": {
        "id": "A-5rO-9Vsc73"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AFmaBwI_teRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Installing all the necessary Python libraries for implementing the RAG model and related components**\n",
        "\n",
        " pinecone[grpc] and langchain-pinecone:\n",
        "For storing and querying vector embeddings in the Pinecone vector database.\n",
        "\n",
        "langchain, langchain-openai, langchain-text-splitters, and langchain-community:\n",
        "For building the RAG pipeline using LangChain, enabling integrations with OpenAI models and text-splitting utilities for document processing."
      ],
      "metadata": {
        "id": "0ffwYhBps2ML"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj2FQRRzdCg-",
        "outputId": "33a42067-4112-4e83-fa74-4e8ceb3234cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-pinecone\n",
            "  Downloading langchain_pinecone-0.2.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.11/dist-packages (0.3.5)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.15)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.7-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.15-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.10.5)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.41.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting pinecone[grpc]\n",
            "  Downloading pinecone-5.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone[grpc]) (2024.12.14)\n",
            "Requirement already satisfied: googleapis-common-protos>=1.53.0 in /usr/local/lib/python3.11/dist-packages (from pinecone[grpc]) (1.66.0)\n",
            "Requirement already satisfied: grpcio>=1.59.0 in /usr/local/lib/python3.11/dist-packages (from pinecone[grpc]) (1.69.0)\n",
            "Collecting lz4>=3.1.3 (from pinecone[grpc])\n",
            "  Downloading lz4-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting pinecone-plugin-inference<4.0.0,>=2.0.0 (from pinecone[grpc])\n",
            "  Downloading pinecone_plugin_inference-3.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone[grpc])\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: protobuf<5.0,>=4.25 in /usr/local/lib/python3.11/dist-packages (from pinecone[grpc]) (4.25.5)\n",
            "Collecting protoc-gen-openapiv2<0.0.2,>=0.0.1 (from pinecone[grpc])\n",
            "  Downloading protoc_gen_openapiv2-0.0.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone[grpc]) (2.8.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from pinecone[grpc]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone[grpc]) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone[grpc]) (2.3.0)\n",
            "Collecting aiohttp<3.11,>=3.10 (from langchain-pinecone)\n",
            "  Downloading aiohttp-3.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone) (0.3.31)\n",
            "Collecting langchain-tests<0.4.0,>=0.3.7 (from langchain-pinecone)\n",
            "  Downloading langchain_tests-0.3.9-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone) (1.26.4)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.59.9)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.27.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.1)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain-pinecone) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain-pinecone) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain-pinecone) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain-pinecone) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain-pinecone) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain-pinecone) (1.18.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.23.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain-pinecone) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<0.4.0,>=0.3.7->langchain-pinecone) (0.28.1)\n",
            "Requirement already satisfied: pytest<9,>=7 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<0.4.0,>=0.3.7->langchain-pinecone) (8.3.4)\n",
            "Collecting pytest-asyncio<1,>=0.20 (from langchain-tests<0.4.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading pytest_asyncio-0.25.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting pytest-socket<1,>=0.6.0 (from langchain-tests<0.4.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading pytest_socket-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting syrupy<5,>=4 (from langchain-tests<0.4.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading syrupy-4.8.1-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone[grpc]) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<0.4.0,>=0.3.7->langchain-pinecone) (1.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain-pinecone) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<0.4.0,>=0.3.7->langchain-pinecone) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<0.4.0,>=0.3.7->langchain-pinecone) (1.5.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<3.11,>=3.10->langchain-pinecone) (0.2.1)\n",
            "Downloading langchain_pinecone-0.2.2-py3-none-any.whl (11 kB)\n",
            "Downloading langchain_openai-0.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.7-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.15-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading streamlit-1.41.1-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_tests-0.3.9-py3-none-any.whl (36 kB)\n",
            "Downloading lz4-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone-5.4.2-py3-none-any.whl (427 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_inference-3.1.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading protoc_gen_openapiv2-0.0.1-py3-none-any.whl (7.9 kB)\n",
            "Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_asyncio-0.25.2-py3-none-any.whl (19 kB)\n",
            "Downloading pytest_socket-0.7.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading syrupy-4.8.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: watchdog, uvicorn, python-multipart, python-dotenv, pypdf, pinecone-plugin-interface, mypy-extensions, marshmallow, lz4, httpx-sse, typing-inspect, tiktoken, syrupy, starlette, pytest-socket, pytest-asyncio, pydeck, protoc-gen-openapiv2, pinecone-plugin-inference, aiohttp, pydantic-settings, pinecone, fastapi, dataclasses-json, streamlit, langchain-tests, langchain-openai, langchain-pinecone, langchain_community\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.11.11\n",
            "    Uninstalling aiohttp-3.11.11:\n",
            "      Successfully uninstalled aiohttp-3.11.11\n",
            "Successfully installed aiohttp-3.10.11 dataclasses-json-0.6.7 fastapi-0.115.7 httpx-sse-0.4.0 langchain-openai-0.3.2 langchain-pinecone-0.2.2 langchain-tests-0.3.9 langchain_community-0.3.15 lz4-4.3.3 marshmallow-3.26.0 mypy-extensions-1.0.0 pinecone-5.4.2 pinecone-plugin-inference-3.1.0 pinecone-plugin-interface-0.0.7 protoc-gen-openapiv2-0.0.1 pydantic-settings-2.7.1 pydeck-0.9.1 pypdf-5.1.0 pytest-asyncio-0.25.2 pytest-socket-0.7.0 python-dotenv-1.0.1 python-multipart-0.0.20 starlette-0.45.3 streamlit-1.41.1 syrupy-4.8.1 tiktoken-0.8.0 typing-inspect-0.9.0 uvicorn-0.34.0 watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install \\\n",
        "pinecone[grpc] \\\n",
        "langchain-pinecone \\\n",
        "langchain-openai \\\n",
        "langchain-text-splitters \\\n",
        "langchain \\\n",
        "fastapi \\\n",
        "uvicorn \\\n",
        "requests \\\n",
        "langchain_community \\\n",
        "pydantic \\\n",
        "streamlit \\\n",
        "python-multipart \\\n",
        "pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PmT1yeRUsYSs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y-DVNfIltplA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d92471d4-ffd4-4982-867f-734a039e33a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n",
            "··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "os.environ[\"PINECONE_API_KEY\"] = getpass.getpass()\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNi-6yZpyEtL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UHtdgrVPyW5L"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ahrwfaI9uNgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing libraries and configuring Pinecone client:\n",
        " This cell initializes the Pinecone vector database by importing necessary libraries and configuring the client for serverless mode**"
      ],
      "metadata": {
        "id": "B0sRYpYzuGjX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nCDIyXdydChB"
      },
      "outputs": [],
      "source": [
        "from pinecone.grpc import PineconeGRPC as Pinecone\n",
        "from pinecone import ServerlessSpec, PodSpec\n",
        "import time\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "use_serverless = True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j2RM8BfguPx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating the index and preparing Pinecone for use:\n",
        "This cell creates a new Pinecone index with specified dimensionality (1536 for OpenAI's text-embedding-ada-002) and sets the similarity metric (e.g., cosine). It also ensures the index is ready before use by continuously checking its status until fully initialized.**"
      ],
      "metadata": {
        "id": "Ln2cUeAguR3H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm6zvFCBdChB",
        "outputId": "60c4b87c-0d35-4155-9258-bb6ad4ec7504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deleted_old\n"
          ]
        }
      ],
      "source": [
        "# configure client\n",
        "pc = Pinecone()\n",
        "if use_serverless:\n",
        "    spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "else:\n",
        "    # if not using a starter index, you should specify a pod_type too\n",
        "    spec = PodSpec()\n",
        "# check for and delete index if already exists\n",
        "index_name = \"sarvam-chat\"\n",
        "items = pc.list_indexes().indexes\n",
        "existing_indexes = [item['name'] for item in items]\n",
        "if index_name in existing_indexes:\n",
        "    pc.delete_index(index_name)\n",
        "    print(\"deleted_old\")\n",
        "# create a new index\n",
        "pc.create_index(\n",
        "    index_name,\n",
        "    dimension=1536,  # dimensionality of text-embedding-ada-002\n",
        "    metric='cosine',\n",
        "    spec=spec\n",
        ")\n",
        "# wait for index to be initialized\n",
        "while not pc.describe_index(index_name).status['ready']:\n",
        "    time.sleep(1)\n",
        "\n",
        "index = pc.Index(index_name)\n",
        "index.describe_index_stats()\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "text_field = \"text\"\n",
        "\n",
        "vectorstore = PineconeVectorStore(\n",
        "    index, embeddings, text_field )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1ydwaWJel0u",
        "outputId": "71da271a-75a1-4733-a2b9-86940f60058a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Approach to Data Extraction and Preprocessing:**"
      ],
      "metadata": {
        "id": "evOg97N7CjCo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Rr7aNkcAyq4L"
      },
      "outputs": [],
      "source": [
        "#with open('iesc111.pdf','r') as file:\n",
        "location = '/content/drive/My Drive/Sample Financial Statement.pdf'\n",
        "pdf_loader = PyPDFLoader(location)\n",
        "\n",
        "pages = pdf_loader.load_and_split()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this step, the financial data from Profit & Loss (P&L) tables stored in a PDF document is loaded using the PyPDFLoader. The PDF file is located on Google Drive, and the content is extracted and split into separate pages. This preprocessing step ensures that the P&L data is prepared for further analysis and embedding.**"
      ],
      "metadata": {
        "id": "IAEW-4uLuxOe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rxv1r_Mty9mD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8663b891-05d4-4755-8b69-e326a971f472"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/drive/My Drive/Sample Financial Statement.pdf', 'page': 0, 'page_label': '1'}, page_content='Index Page No.\\nCondensed Consolidated Balance Sheet ……………………………………………………………………………………………………………………..1\\nCondensed Consolidated Statement of Profit and Loss ……………………………………………………………………………………………………………………..2\\nCondensed Consolidated Statement of Changes in Equity ……………………………………………………………………………………………………………………..3\\nCondensed Consolidated Statement of Cash Flows ……………………………………………………………………………………………………………………..5\\nOverview and Notes to the Interim Condensed Consolidated Financial Statements\\n1. Overview\\n1.1 Company overview ……………………………………………………………………………………………………………………..7\\n1.2 Basis of preparation of financial statements ……………………………………………………………………………………………………………………..7\\n1.3 Basis of consolidation ……………………………………………………………………………………………………………………..7\\n1.4 Use of estimates and judgments ……………………………………………………………………………………………………………………..7\\n1.5 Critical accounting estimates and judgments……………………………………………………………………………………………………………………..8\\n2. Notes to the Interim Condensed Consolidated Financial Statements\\n2.1 Business Combinations ……………………………………………………………………………………………………………………………………………..10\\n2.2 Property, plant and equipment ……………………………………………………………………………………………………………………..12\\n2.3 Goodwill and other intangible assets……………………………………………………………………………………………………………………..14\\n2.4 Investments ……………………………………………………………………………………………………………………………………………..15\\n2.5 Loans ……………………………………………………………………………………………………………………………………………..16\\n2.6 Other financial assets ……………………………………………………………………………………………………………………………………………..16\\n2.7 Trade receivables ……………………………………………………………………………………………………………………………………………..16\\n2.8 Cash and cash equivalents ……………………………………………………………………………………………………………………………………………..17\\n2.9 Other assets ……………………………………………………………………………………………………………………………………………..17\\n2.10 Financial instruments ……………………………………………………………………………………………………………………………………………..18\\n2.11 Equity ……………………………………………………………………………………………………………………………………………..22\\n2.12 Other financial liabilities ……………………………………………………………………………………………………………………………………………………..25\\n2.13 Other liabilities ……………………………………………………………………………………………………………………………………………..25\\n2.14 Provisions ……………………………………………………………………………………………………………………………………………..26\\n2.15 Income taxes ……………………………………………………………………………………………………………………………………………..27\\n2.16 Revenue from operations ……………………………………………………………………………………………………………………………………………..28\\n2.17 Other income, net ……………………………………………………………………………………………………………………………………………..30\\n2.18 Expenses ……………………………………………………………………………………………………………………………………………..31\\n2.19 Leases ……………………………………………………………………………………………………………………………………………..32\\n2.20 Basic and diluted shares used in computing earnings per equity share ……………………………………………………………………………………………………………………………………………..34\\n2.21 Contingent liabilities and commitments  ……………………………………………………………………………… 34\\n2.22 Related party transactions ……………………………………………………………………………………………………………………………………………..36\\n2.23 Segment reporting ……………………………………………………………………………………………………………………………………………..37\\n2.24 Function wise classification of Condensed Consolidated Statement of Profit and Loss ……………………………………………………………………………………………………………………………………………..39\\nINFOSYS LIMITED AND SUBSIDIARIES\\nCondensed Consolidated Financial Statements under \\nIndian Accounting Standards (Ind AS) \\nfor the three months and year ended March 31, 2024')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "pages[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HhMqVaFOv7bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Once the P&L data is extracted, it is embedded using the OpenAIEmbeddings model. This model converts the financial terms, metrics, and other information from the document into high-dimensional vectors that capture the semantic meaning of the data. These embeddings are stored in Pinecone, a vector database, allowing efficient similarity searches and retrieval of relevant information based on user queries. Like here we are adding the documments to our vectorstore**"
      ],
      "metadata": {
        "id": "nOy3SrXjv6MW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Whg7TyfodChD",
        "outputId": "d6518a16-5e38-4a19-b790-76141ed0a3cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['50d7dc6a-e95f-443b-8eee-8323040c6589',\n",
              " '068037fc-eaa6-4844-bd5d-9df597863665',\n",
              " 'b28373cd-e917-4dcd-ae43-70dbadc0882b',\n",
              " 'b05dcdd4-bb38-45d9-981c-3bcea0022d17',\n",
              " '3fdf5380-c289-4f9f-9220-3ff44014a155',\n",
              " '4669e453-4a3e-4107-ae43-05063a2052c8',\n",
              " 'c1d7cb2e-a489-4d12-9bc9-34f938d2a989',\n",
              " '789395a3-fea8-4a27-9e0e-d4385b14e899',\n",
              " 'e073ddf2-fa21-43c0-b457-7b4718ad109e',\n",
              " '536831e4-4305-49c2-81f3-0abc3300fbb7',\n",
              " '18b5642c-55f0-4050-8efd-e80c0568c34c',\n",
              " 'e5f569ee-17ed-4d51-8d3e-4505fab4ee70',\n",
              " '48d6eebe-3933-4838-afd8-4c0a6b01a4f6',\n",
              " '38292caf-9b92-46ed-a25a-d1fe4cff8af1',\n",
              " '10abd77f-348e-428f-9ce9-5347cce908a5',\n",
              " '028aa116-e25b-44d8-8ba9-87216ac28ca3',\n",
              " 'ea143e38-90a7-4722-9074-9ef797a338c7',\n",
              " '997c73cc-25e5-4e46-9717-5e16a336347a',\n",
              " 'ceeeb320-066a-4922-bc11-4a242170fc5f',\n",
              " 'db25a165-44f4-43be-a508-efec449da329',\n",
              " 'ac3feb41-862a-4b49-9d8f-274aaac502a0',\n",
              " '8ca7257c-bc76-48f9-b50c-56b329f8070f',\n",
              " 'eaf0e7d5-2a52-4be9-a7b0-e54d6d4502ef',\n",
              " 'af56f6d8-485c-4640-86eb-634583016133',\n",
              " 'fefb4773-7eb8-49e3-aec9-72657a0c8f99',\n",
              " '2f824d31-edff-456a-ad46-dffc2f6c1fec',\n",
              " 'c62556bd-f9b3-466b-bb28-f9774d03136d',\n",
              " '13f095cd-71e5-43e2-bb75-e11cb816bc87',\n",
              " '9ea4ba61-a6df-461c-895e-6a0bb0249eaf',\n",
              " '0bfb2aed-0054-400b-9f08-c12f73fac541',\n",
              " 'e939bf83-52bb-4431-8327-747480d233b3',\n",
              " 'b7575dde-ad46-4e74-9a19-272e58c7671f',\n",
              " '041a03d2-574f-4e6a-a0c9-9e4e606d9549',\n",
              " '2c758d54-ceb8-4acd-8ed8-9a315fec554a',\n",
              " 'b572c6bc-f448-4d28-999e-2ff8ae12c407',\n",
              " 'ae2a4a8b-eb6f-48f5-b1ab-a544de01d7d4',\n",
              " 'f11e9a8c-b6e6-4313-a666-237ff4e4de8e',\n",
              " 'a051eeb3-c1b7-4803-8504-eaef0b500c7b',\n",
              " 'e2226bef-27cb-4fd0-be4d-e8bc51ffec8d',\n",
              " 'f038b3c4-ae2c-4afb-9101-46ac74b4c3ba',\n",
              " 'b181a2bc-6e2e-43d6-a0f5-d6ec67c389f8',\n",
              " '182d014d-0bf2-44a3-be57-9d25f9cf843d',\n",
              " '47a3e5a8-b3b6-423a-b9f3-72d08114f2ef',\n",
              " '37f69d90-78cc-4672-86b4-415563d77d0a',\n",
              " '003818c4-6305-43d1-864b-3e1f37b8e05b',\n",
              " '13a9d94f-2de1-4f75-a71a-ea9c3cd10ebe',\n",
              " '32bf4e82-db0a-4452-affb-6cb1027b6ac7',\n",
              " '6efe3c9b-bf0d-4d55-a462-fbd72e85db13',\n",
              " 'dbd77bdf-be0b-4b36-bfe1-88bdaa370a9c',\n",
              " '63fb6edf-335a-4b9c-8a41-42877258cf83',\n",
              " '0485e576-10d0-4f36-afc5-f7f8dffeaad6',\n",
              " '4188d43d-b6ac-464d-ba96-7b04415209e5',\n",
              " '17126396-e0ed-4309-b38f-067847a16101',\n",
              " '2f56cd5e-8170-4833-8d54-950ba157191a',\n",
              " 'a79d46a6-1544-4afb-abde-9b63dc1c603f',\n",
              " '7cb7541b-9af2-4831-a8cf-9b8a6b361496',\n",
              " '85aec4be-4e86-4330-a992-c6e797aec213',\n",
              " '419b6541-1063-40cf-8b6f-e16e2c6568c4',\n",
              " '6670cec3-4f0a-446c-888f-7a807ca2cbc9',\n",
              " '0974955f-0ddc-4af6-85c2-cb035bddd0a6',\n",
              " '4ae35771-3bab-4928-8d9b-1039ca779f98',\n",
              " 'c6a25297-be8d-4438-a1fb-df7b8aacf077',\n",
              " '47f2ebec-01d9-4c07-9290-84f6bb278ba2',\n",
              " '9f74873a-8142-4af0-9575-206cfbf094a9',\n",
              " '01b0003e-4d59-40d1-b46d-f9f0aae69265',\n",
              " 'bc76778f-e3d3-4c83-b3a2-33f5414b299d',\n",
              " 'bd28f49e-58e3-4cc2-a9d1-4ae0a9b081d1',\n",
              " '41618f24-79fb-49e9-b73a-7d214a97def8',\n",
              " 'ed2db27b-8d29-49e1-b6cf-b0efdbe6377e',\n",
              " '614bf025-aff1-450c-bcd3-3da5ef3bf1c2',\n",
              " '1968d899-e133-46f6-92a4-ca050501aba9',\n",
              " 'bf7caf82-56cf-427c-a557-4ff34faae30f',\n",
              " 'bfa7a66c-112e-4329-b999-f48fbe038b75',\n",
              " '0335a1ee-7d5a-4944-afe2-0eff51827788',\n",
              " '1ad2a81e-22fd-4f9a-aedf-a77bb4bdb4e3',\n",
              " '360ef2aa-c2d5-4bf0-b44a-ca21767f9f6c']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# async_req is required to make sure it is updated in real time\n",
        "vectorstore.add_documents(documents= pages, async_req=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uPT6AjwkxhMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query Definition: The variable query specifies the search query (in this case, \"What is the book about?\").\n",
        "Search Operation: The vectorstore.similarity_search(query) retrieves documents from the vector store that are most similar to the query based on their embeddings.\n",
        "Output (similar_docs): It stores the retrieved documents in similar_docs, which can then be used as contextual input for further processing, such as generating responses.**"
      ],
      "metadata": {
        "id": "S0CqSK-_xiA6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "x7SHlfD43fN0"
      },
      "outputs": [],
      "source": [
        "query = \"What is the book about?\"\n",
        "\n",
        "similar_docs = vectorstore.similarity_search(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3NtGloln3tq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fcf98b9-b55a-4c0a-af5c-48945b98b865"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "similar_docs"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pufm5rcHw5e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Before passing the retrieved documents to the generative model,we format them into a readable form. This step ensures that the context is structured properly so that the model can understand and use it effectively to generate a meaningful response. The documents are joined together into a coherent format to serve as the input context for the RAG model.**"
      ],
      "metadata": {
        "id": "p17zT6lyw7PF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6i2OPwJMdChE"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A9eMq_AlxML9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retrieval-Augmented Generation (RAG) pipeline to answer the question, \"What is the gross profit for Q3 2024?\". The process involves a retriever that searches for the top 6 relevant documents from the vector store based on the query.**\n",
        "\n",
        "**A prompt template defines the role of the model (a financial analyst) and instructs it to answer only based on the provided context, otherwise responding with an apology if insufficient information is available**\n",
        "\n",
        "**Finally, the RAG chain combines these steps: it retrieves relevant documents, formats them, and uses the GPT-4 model to generate a response based on the prompt and the retrieved context.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ekj2Eoz-xMjz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "md6U73qqdChE"
      },
      "outputs": [],
      "source": [
        "question = \"What is the gross profit for Q3 2024? How do the net income and operating expenses compare for Q1 2024?\"\n",
        "prompt = \"\"\"You are a Financial Analyst. You answer queries based on Profit/Loss context provide to you.\n",
        "    Use relevant information related to income, expenses, profit margins, and other key financial metrics from the provided context and generate accurate and coherent responses.\n",
        "    Only provide answer based on the context information, else reply \"I apologise. I do not have enough information to answer your query.\"\n",
        "    \"\"\"\n",
        "retriever = vectorstore.as_retriever(search_kwargs={'k': 6})\n",
        "prompt_rag = (\n",
        "    PromptTemplate.from_template(prompt+\"Answers given question: {question}. \\nContext: {context}. In generated response provide reference to metadata from which context and table name along with column or row number or column and row name used in generating response\")\n",
        ")\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt_rag\n",
        "    | llm\n",
        ")\n",
        "\n",
        "\n",
        "res = rag_chain.invoke(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lastly we are printing our results**"
      ],
      "metadata": {
        "id": "sOo0ta4GzSkY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsSyVFdVdChE",
        "outputId": "50e376e2-2f0a-4698-c117-51193611fa43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. **What is the gross profit for Q3 2024?**\n",
            "\n",
            "   The gross profit for Q3 2024 is ₹11,175 crore. This information is derived from the \"FUNCTION WISE CLASSIFICATION OF CONDENSED CONSOLIDATED STATEMENT OF PROFIT AND LOSS\" table, specifically from the row labeled \"Gross profit\" under the column for the year 2024.\n",
            "\n",
            "2. **How do the net income and operating expenses compare for Q1 2024?**\n",
            "\n",
            "   For Q1 2024, the net income is ₹7,975 crore, while the total operating expenses are ₹3,554 crore. This indicates that the net income is significantly higher than the operating expenses. The net income is found in the \"FUNCTION WISE CLASSIFICATION OF CONDENSED CONSOLIDATED STATEMENT OF PROFIT AND LOSS\" table, under the row \"Profit for the period\" for the year 2024, and the operating expenses are found in the same table under the row \"Total operating expenses\" for the year 2024.\n"
          ]
        }
      ],
      "source": [
        "print(res.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KDEK05_A4Prn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c00785b7-a637-46af-cf22-b0712f7df667"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What is the book about?',\n",
              " 'result': 'The document appears to be a set of financial statements and notes for Infosys Limited and its subsidiaries, prepared under Indian Accounting Standards (Ind AS) for the three months and year ended March 31, 2024. It includes various financial statements such as the balance sheet, statement of profit and loss, statement of changes in equity, and statement of cash flows, along with detailed notes on different financial aspects like business combinations, property, plant and equipment, financial instruments, and more.'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")\n",
        "\n",
        "qa.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FXyG3ibQfv_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DELIVERABLES**"
      ],
      "metadata": {
        "id": "sO054Q30z2DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Architecture:**\n",
        "\n",
        "**The architecture is based on a Retrieval-Augmented Generation (RAG) model. It consists of the following key components:**\n",
        "\n",
        "**Vector Embeddings: Financial terms and data points from the Profit & Loss (P&L) table are embedded into vector representations using the OpenAI embedding model. These embeddings capture the semantic meaning of the terms and metrics, allowing the system to efficiently search for related content.**\n",
        "\n",
        "**Vector Database (Pinecone): The embeddings are stored in a Pinecone vectorstore. This allows efficient similarity searches across large datasets of financial terms, enabling quick retrieval of relevant data when a query is made.**\n",
        "\n",
        "**Retriever: The retriever searches Pinecone's vectorstore to find relevant documents based on the query. It retrieves the most relevant data points, which are then formatted to provide context for the model.**\n",
        "\n",
        "**Generative Model: The GPT-4 model is used for generating responses. The model receives a prompt along with the context retrieved from Pinecone, and it generates a coherent response based on the given data.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L5B1g7lqz15D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**How the Generative Responses are Created:**\n",
        "\n",
        "**Query Processing:\n",
        "When a user asks a question (e.g., \"What is the gross profit for Q3 2024?\"), the query is passed to the retriever, which uses Pinecone's similarity search to find the most relevant documents from the P&L data.**\n",
        "\n",
        "**Context Generation:\n",
        "The retrieved documents are then formatted into a cohesive context, which is combined with a prompt instructing the model to act as a financial analyst.**\n",
        "\n",
        "**The formatted context and the user query are sent to the GPT-4 model. The model processes this information and generates a response based on the context provided. The model is trained to answer financial queries accurately using the data points retrieved from the P&L table.**"
      ],
      "metadata": {
        "id": "BqbjZi-kz1sw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Challenges Encountered and Solutions Implemented:**\n",
        "\n",
        "**Challenge: Extracting structured data from PDFs**\n",
        "\n",
        "**Solution: The initial challenge was extracting data specially tabular data, in a structured manner from PDFs, as PDFs don't have a standardized format. Using tools like PyPDFLoader helped extract and split the text efficiently, although the formatting of the data still needed preprocessing for use in vector databases.**\n",
        "\n",
        "**Challenge: Efficiently handling large datasets**\n",
        "\n",
        "**Solution: Storing embeddings in Pinecone's vectorstore allowed efficient search and retrieval, ensuring that the system can handle large amounts of financial data without compromising on performance.**\n",
        "\n",
        "**Challenge: Ensuring relevance in search results**\n",
        "\n",
        "**Solution: By tweaking the parameters of the similarity search and adjusting the number of documents retrieved (k=6), the model was fine-tuned to retrieve the most relevant data for each query.**\n",
        "\n",
        "**Challenge: Generating accurate and coherent financial responses**\n",
        "\n",
        "**Solution: The model's prompt was carefully crafted to ensure that it could generate responses grounded in the provided financial context. Explicit instructions in the prompt helped ensure that the responses remained relevant and accurate, based on the data available.**\n"
      ],
      "metadata": {
        "id": "HMrYe_oTz1gB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XF6Em8wkSMHd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}